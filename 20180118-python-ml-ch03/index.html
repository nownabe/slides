<!doctype html>
<html>
<head>
  <meta charset="utf-8">

  <title>Python機械学習プログラミング 第3章</title>
  <meta name="description" content="Python機械学習プログラミング 第3章">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta name="author" content="nownabe">

  <link rel="stylesheet" href="css/reveal.min.css">
  <link rel="stylesheet" href="css/theme/white.css" id="theme">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Theme used for syntax highlighting of code -->
  <link rel="stylesheet" href="lib/css/zenburn.css">

  <script>
		const link = document.createElement("link");
		link.rel = "stylesheet";
		link.type = "text/css";
		link.href = window.location.search.match(/print-pdf/gi) ? "css/print/pdf.css" : "css/print/paper.css";
		document.getElementsByTagName("head")[0].appendChild(link);
	</script>

  <!--[if lt IE 9]>
  <script src="lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
<div class="reveal">
<div class="slides">

  <section data-markdown>
    <script type="text/template">
# Python 機械学習プログラミング

機械学習勉強会

2018/01/18 @nownabe
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 範囲
* 第3章 分類問題 - 機械学習ライブラリ scikit-learn の活用
  * 3.4 サポートベクトルマシンによる最大マージン分類
  * 3.5 カーネルSVMを使った非線形問題の求解
  * 3.6 決定木学習
  * 3.7 k近傍法: 怠惰学習アルゴリズム
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.4 サポートベクトルマシンによる最大マージン分類
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# サポートベクトルマシン
* 広く利用されている強力な機械学習アルゴリズム
* 学習サンプルと決定境界の距離を最大化して学習
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.4.1 最大マージンを直感的に理解する
* 決定境界 $b+\boldsymbol{w}^T\boldsymbol{x}=0$ を学習
* 最適化(最小化)
  * 目的関数: $\frac{1}{2}||\boldsymbol{w}||^2$
  * 制約条件: $y^{(i)}(b+\boldsymbol{w}^T\boldsymbol{x})\geq 1 \forall_i$
* 二次計画法で解けるらしい
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.4.2 スラック変数を使った非線形分離可能なケースへの対処
* さっきの最適化問題は線形分離可能なケースのみ解ける
* スラック変数:
  * 線形分離不可能なケースへの対処
  * バイアスとバリアンスのトレードオフ調整

\\begin{eqnarray}
\frac{1}{2}||\boldsymbol{w}||^2+C\left(\sum_i\xi^{(i)}\right)
\\end{eqnarray}

    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.4.3 scikit-learn での代替実装
* 内部でliblinearやlibsvmを使用
* データが大きい場合の代替としてSGDClassifier
  * 様々なアルゴリズムを代替可能
  * オンライン学習も可能
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.5 カーネルSVMを使った非線形問題の求解
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# カーネル化
* 線形分離できないデータを高次元に写像
* 高次元空間で線形分離
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.5.1 カーネルトリックを使って分離超平面を高次元空間で特定する
* 高次元データを写像するのは高コスト
* SVMでは実際に写像を計算する必要がない
* 内積計算だけでOK
* ハイパーパラメータ $\gamma$
  * 大きいとモデルは複雑
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.6 決定木学習
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 決定木
* モデルの理解が容易
* データに対する質問を繰返し分類
* 情報利得が最大になるような質問を学習
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.6.1 情報利得の最大化: できるだけ高い効果を得る
* 情報利得
  * 親ノードの不純度が高く、子ノードの不純度が低いとき、情報利得は大きい

\\begin{eqnarray}
IG(D_p, f) = I(D_p) - \sum_j\frac{N_j}{N_p}I(D_j)
\\end{eqnarray}

    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 不純度
* 不純度 = ごった煮具合
* 不純度の指標
  * エントロピー
  * ジニ不純度
  * 分類誤差
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.6.2 決定木の構築
* scikit-learnでは学習した決定木を画像に出力可能
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.6.3 ランダムフォレストを使って弱い学習アルゴリズムと強い学習アルゴリズムを結合する
* ランダムフォレスト
  * 分類性能が高い
  * スケーラビリティがある
  * 使いやすい
  * 非常に支持されている
* 弱い決定木を大量に作って多数決
  * データをサンプリング
  * 特徴量もサンプリング
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# 3.7 k近傍法: 怠惰学習アルゴリズム
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# k近傍法
* 怠惰学習
  * 学習しない
  * 学習データを丸暗記
* 分類したいデータの最近傍点k個のラベルで多数決
* 次元数が少ない場合KD木が有効
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
# KD木 k-dimensional tree
* ユークリッド空間
* 最近傍点を効率的に探索するデータ構造
* $N >> 2^D$ のとき有効
  * サンプル数$N$
  * 次元数$D$
* $O(\log N)$で最近傍点を探索可能
    </script>
  </section>

  <section data-markdown>
    <script type="text/template">
おしまい
    </script>
  </section>

</div>
</div>
<script src="lib/js/head.min.js"></script>
<script src="js/reveal.min.js"></script>
<script src="config.js"></script>
</body>
</html>
